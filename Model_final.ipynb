{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "raSk3cevGV4j"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "r8uImLgoGV4m"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"dane-treningowe.csv\",index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a8MkvIOsSt74"
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "    data: Sequence of observations as a list or NumPy array.\n",
    "    n_in: Number of lag observations as input (X).\n",
    "    n_out: Number of observations as output (y).\n",
    "    dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "    Pandas DataFrame of series framed for supervised learning.\n",
    "    source: https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vskAjnOcpWo5"
   },
   "outputs": [],
   "source": [
    "def df_lag(i,o,X_train,X_test,Y_train,Y_test,lstm):\n",
    "    X_train=series_to_supervised(X_train,i,o).values\n",
    "    X_test=series_to_supervised(X_test,i,o).values\n",
    "    Y_train=Y_train[i:len(Y_train)-o+1]\n",
    "    Y_test=Y_test[i:len(Y_test)-o+1]\n",
    "    oversample = RandomOverSampler(sampling_strategy=0.3)\n",
    "    X_train, Y_train = oversample.fit_resample(X_train, Y_train)\n",
    "    if lstm==True:\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    return [X_train,X_test,Y_train,Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rP-7rBtTQbL-"
   },
   "outputs": [],
   "source": [
    "def data_cl(data,n_in,n_out,turbine,lstm=False,time_window=72):\n",
    "    data=data.drop(columns=[\"Unnamed: 0\",\"Time_ID\"])\n",
    "    data=data[data.Turbine_ID==turbine]\n",
    "    data[\"Efficiency\"]=round(data['Grd_Prod_Pwr_Avg']/data['Predicted_power'],2)\n",
    "    columns=['Grd_Prod_Pwr_Avg', 'Amb_WindSpeed_Avg', 'Amb_Temp_Avg',\n",
    "           'Nac_Temp_Avg', 'Rtr_RPM_Avg', 'Gear_Bear_Temp_Avg', 'Gen_RPM_Avg',\n",
    "           'Gen_Bear2_Temp_Avg', 'Gen_Bear_Temp_Avg', 'Amb_WindDir_Relative_Avg']\n",
    "    data.loc[:,columns]=1-data.loc[:,columns].div(data[\"Predicted_power\"],axis=0)\n",
    "    data[\"Failure_window\"]=0\n",
    "    data.loc[data[data.Time_to_failure<time_window].index,\"Failure_window\"]=1\n",
    "    data.sort_index(inplace=True)\n",
    "    X=data.iloc[:,[1,2,3,4,5,6,7,8,9,10,16,17]]\n",
    "    y=data.iloc[:,18]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    df=df_lag(n_in,n_out,X_train,X_test,y_train,y_test,lstm)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trenowanie na peÅ‚nym zbiorze treningowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:17:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:18:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:19:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:20:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:21:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "turbines=data.Turbine_ID.unique()\n",
    "turbines.sort()\n",
    "models=[]\n",
    "for turbine in list(turbines):\n",
    "    data_=data_cl(data,30,6,turbine,False,72)\n",
    "    X_train=data_[0]\n",
    "    X_test=data_[1]\n",
    "    y_train=data_[2]\n",
    "    y_test=data_[3]\n",
    "    X_train=np.concatenate((X_train,X_test))\n",
    "    y_train=np.concatenate((y_train,y_test))\n",
    "    model=xgb.XGBClassifier(eta=0.1,n_estimators=100,max_depth=2,gamma=1)\n",
    "    model.fit(X_train,y_train)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=pd.read_csv(\"dane-testowy.csv\",index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cl_test(data,n_in,n_out,turbine,lstm=False):\n",
    "    data=data.drop(columns=[\"Unnamed: 0\",\"Time_ID\"])\n",
    "    data=data[data.Turbine_ID==turbine]\n",
    "    data[\"Efficiency\"]=round(data['Grd_Prod_Pwr_Avg']/data['Predicted_power'],2)\n",
    "    columns=['Grd_Prod_Pwr_Avg', 'Amb_WindSpeed_Avg', 'Amb_Temp_Avg',\n",
    "           'Nac_Temp_Avg', 'Rtr_RPM_Avg', 'Gear_Bear_Temp_Avg', 'Gen_RPM_Avg',\n",
    "           'Gen_Bear2_Temp_Avg', 'Gen_Bear_Temp_Avg', 'Amb_WindDir_Relative_Avg']\n",
    "    data.loc[:,columns]=1-data.loc[:,columns].div(data[\"Predicted_power\"],axis=0)\n",
    "    data.sort_index(inplace=True)\n",
    "    X=data.iloc[:,[1,2,3,4,5,6,7,8,9,10,14,15]]\n",
    "    X_test=series_to_supervised(X,n_in,n_out).values\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prognoza awarii z 12 godzinnym wyprzedzeniem na zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "predictions=[]\n",
    "for turbine in list(turbines):\n",
    "    X_test=data_cl_test(data_test,30,6,turbine,False)\n",
    "    y_test=models[i].predict(X_test)\n",
    "    predictions.append(y_test)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for turbine in turbines:\n",
    "    pred=pd.DataFrame(predictions[i],columns=[turbines[i]])\n",
    "    pred.to_csv(\"prediction_\"+turbine+\".csv\",index=False)\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "modelowanie (2) (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
